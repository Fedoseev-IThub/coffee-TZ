

import os
from pathlib import Path


from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate


def load_vector_store(persist_directory: str = "./vector_db"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Args:
        persist_directory: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
    """
    if not os.path.exists(persist_directory):
        raise FileNotFoundError(
            f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {persist_directory}.\n"
            f"–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ generate_vectors.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤."
        )
    
    embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-base",
        model_kwargs={'device': 'cpu'}
    )
    
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    return vectorstore

def create_qa_chain(vectorstore, use_local_llm: bool = True):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ü–µ–ø–æ—á–∫—É –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    
    Args:
        vectorstore: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        use_local_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (Ollama) –∏–ª–∏ OpenAI
    """

    prompt_template = """–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
–ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:"""
    
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    

    if use_local_llm:

        try:
            llm = Ollama(model="llama2", temperature=0.7)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.ai/")
            print("–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ OpenAI, —É—Å—Ç–∞–Ω–æ–≤–∏–≤ OPENAI_API_KEY")
            raise
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å."
            )
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=api_key
        )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(
            search_kwargs={"k": 3}
        ),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    
    return qa_chain

def query_rag(question: str, qa_chain):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ RAG-—Å–∏—Å—Ç–µ–º–µ.
    
    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        qa_chain: –¶–µ–ø–æ—á–∫–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
    """
    print(f"\n–í–æ–ø—Ä–æ—Å: {question}")
    print("–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
    
    result = qa_chain({"query": question})
    
    print("\n" + "="*60)
    print("–û—Ç–≤–µ—Ç:")
    print("="*60)
    print(result["result"])
    
    print("\n" + "="*60)
    print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
    print("="*60)
    for i, doc in enumerate(result["source_documents"], 1):
        print(f"\n[{i}] –§—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –¢–ó:")
        print("-" * 60)
        print(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
        print("-" * 60)
    
    return result

def interactive_mode(qa_chain):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –∑–∞–¥–∞–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤."""
    print("\n" + "="*60)
    print("RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∑–∞–¥–∞–Ω–∏–µ–º")
    print("="*60)
    print("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ –¢–ó (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):\n")
    
    while True:
        question = input("> ").strip()
        
        if question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if not question:
            continue
        
        try:
            query_rag(question, qa_chain)
            print("\n" + "-"*60 + "\n")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}\n")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import sys
    
    use_local = True
    if "--openai" in sys.argv:
        use_local = False
    
    try:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        vectorstore = load_vector_store()
        print("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ RAG-—Ü–µ–ø–æ—á–∫–∏...")
        qa_chain = create_qa_chain(vectorstore, use_local_llm=use_local)
        print("RAG-—Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
        
        if len(sys.argv) > 1 and not sys.argv[1].startswith("--"):
            question = " ".join([arg for arg in sys.argv[1:] if not arg.startswith("--")])
            query_rag(question, qa_chain)
        else:
            interactive_mode(qa_chain)
            
    except FileNotFoundError as e:
        print(f"{e}")
        print("\nüí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python generate_vectors.py")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()

